{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethanwcooper/gRPCAuthWithAzureAD/blob/master/EWC180001_POS_Tagger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB_MuuS5KuR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa8adfe-0c1b-49ea-da74-a58b61a91984"
      },
      "source": [
        "# install necessary packages using pip\n",
        "!pip install keras numpy wget"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.9/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzalmizo9Rov",
        "outputId": "088510c9-0d39-4c61-9211-3d655aa3d2a1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "def load_corpus(path):\n",
        "    \"\"\" Load corpus from a folder / directory\n",
        "\n",
        "    Arg:\n",
        "        path: a text sequence denotes the path of corpus\n",
        "\n",
        "    Return:\n",
        "        sentences: a list of sentences that are preprocessed in the corpus\n",
        "    \"\"\"\n",
        "    # Check if the path is a directory.\n",
        "    if not os.path.isdir(path):\n",
        "        sys.exit(\"Input path is not a directory\")\n",
        "    else:\n",
        "        sentences = []\n",
        "        # print(os.listdir(path))\n",
        "        for file in os.listdir(path):\n",
        "            if not os.path.isdir(file):\n",
        "                with open(os.path.join(path, file), 'r') as f:      \n",
        "                    lines = f.readlines()\n",
        "                for i in range(len(lines)):\n",
        "                    lines[i] = lines[i].strip()\n",
        "                    word_tags = lines[i].split()\n",
        "                    sentence = []\n",
        "                    for j in range(len(word_tags)):\n",
        "                        # word_tag_tuple = tuple(word_tags[j].split('/'))\n",
        "                        word_tag_tuple = word_tags[j].split('/')\n",
        "                        word_tag_tuple[0] = word_tag_tuple[0].lower()\n",
        "                        word_tag_tuple = tuple(word_tag_tuple)\n",
        "                        sentence.append(word_tag_tuple)\n",
        "                    if sentence:  # VERIFY THAT LEAVING OUT BLANK LINES IS OK\n",
        "                        sentences.append(sentence)\n",
        "        return sentences\n"
      ],
      "metadata": {
        "id": "sOnNOUAyWM8E"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v59AkYGUcb0a",
        "outputId": "73a8ab42-b978-4dc7-d5bc-ff983c6676b3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT37l9LMoGYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab041d0b-366c-4d49-bbb0-f6a375310307"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_corpus(path):\n",
        "    \"\"\" Load corpus from a folder / directory\n",
        "\n",
        "    Arg:\n",
        "        path: a text sequence denotes the path of corpus\n",
        "\n",
        "    Return:\n",
        "        sentences: a list of sentences that are preprocessed in the corpus\n",
        "    \"\"\"\n",
        "    # Check if the path is a directory.\n",
        "    if not os.path.isdir(path):\n",
        "        sys.exit(\"Input path is not a directory\")\n",
        "    else:\n",
        "        sentences = []\n",
        "        # print(os.listdir(path))\n",
        "        for file in os.listdir(path):\n",
        "            if not os.path.isdir(file):\n",
        "                with open(os.path.join(path, file), 'r') as f:      \n",
        "                    lines = f.readlines()\n",
        "                for i in range(len(lines)):\n",
        "                    lines[i] = lines[i].strip()\n",
        "                    word_tags = lines[i].split()\n",
        "                    sentence = []\n",
        "                    for j in range(len(word_tags)):\n",
        "                        word_tag_tuple = word_tags[j].split('/')\n",
        "                        word_tag_tuple[0] = word_tag_tuple[0].lower()\n",
        "                        word_tag_tuple = tuple(word_tag_tuple)\n",
        "                        sentence.append(word_tag_tuple)\n",
        "                    if sentence:  # VERIFY THAT LEAVING OUT BLANK LINES IS OK\n",
        "                        sentences.append(sentence)\n",
        "        return sentences\n",
        "\n",
        "class HMMTagger:\n",
        "    def __init__(self):\n",
        "        self.tags = ['DETERMINER', 'NOUN', 'ADJECTIVE', 'VERB', 'PREPOSITION', 'PUNCT', 'ADVERB', 'NUMBER', 'PRONOUN', 'CONJUNCTION', 'X']\n",
        "        # self.tags = ['NOUN', 'PUNCT', 'VERB', 'X']\n",
        "\n",
        "        self.init_tag_count = {}\n",
        "        self.num_sentences = 0\n",
        "\n",
        "        self.trans_count = {}\n",
        "        self.num_trans = {}\n",
        "        self.emission_count = {}\n",
        "        self.vocab = set()\n",
        "        self.word_count = {}\n",
        "        self.tag_count = {}\n",
        "\n",
        "    def get_vocab(self, sentences):\n",
        "        vocab = set()\n",
        "        for sentence in sentences:\n",
        "            for pair in sentence:\n",
        "                word = pair[0]\n",
        "                vocab.add(word)\n",
        "        return vocab\n",
        "\n",
        "    def initialize_probabilities(self, sentences):\n",
        "        \"\"\" Initialize / learn probabilities from the corpus\n",
        "\n",
        "        In this function, you should learn inital probability, transition\n",
        "        probability, and emission probability. Also, you should apply the\n",
        "        add-one smoothing properly here.\n",
        "\n",
        "        Arg:\n",
        "            sentences: a list of sentences that are preprocessed in the corpus\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize tag probabilities\n",
        "        self.init_tag_count = {}\n",
        "        self.num_sentences = 0\n",
        "        for sentence in sentences:\n",
        "            start_tag = sentence[0][1]\n",
        "            self.init_tag_count[start_tag] = self.init_tag_count.get(start_tag, 0) + 1\n",
        "            self.num_sentences += 1\n",
        "        # Initialize transistion probabilities\n",
        "        self.trans_count = {}\n",
        "        self.num_trans = {}\n",
        "        for sentence in sentences:\n",
        "            for i in range(1, len(sentence)):\n",
        "                t_curr = sentence[i][1]\n",
        "                t_prev = sentence[i-1][1]\n",
        "                i_to_j = (t_prev, t_curr)\n",
        "                self.trans_count[i_to_j] = self.trans_count.get(i_to_j, 0) + 1\n",
        "                self.num_trans[t_prev] = self.num_trans.get(t_prev, 0) + 1\n",
        "        # Initialize emission probabilities\n",
        "        self.emission_count = {}\n",
        "        self.vocab = self.get_vocab(sentences)\n",
        "        self.word_count = {}\n",
        "        self.tag_count = {}\n",
        "        for sentence in sentences:\n",
        "            for pair in sentence:\n",
        "                word = pair[0]\n",
        "                tag = pair[1]\n",
        "                t_to_w = (tag, word)\n",
        "                self.emission_count[t_to_w] = self.emission_count.get(t_to_w, 0) + 1\n",
        "                self.tag_count[tag] = self.tag_count.get(tag, 0) + 1\n",
        "                self.word_count[word] = self.word_count.get(word, 0) + 1\n",
        "    \n",
        "    def get_init_tag_prob(self, tag):\n",
        "        return (self.init_tag_count.get(tag, 0) + 1) / (self.num_sentences + len(self.tags))\n",
        "    \n",
        "    def get_trans_prob(self, i, j):\n",
        "        i_to_j = (i, j)\n",
        "        return (self.trans_count.get(i_to_j, 0) + 1) / (self.num_trans.get(i, 0) + len(self.tags))\n",
        "\n",
        "    def get_emission_prob(self, tag, word):\n",
        "        t_to_w = (tag, word)\n",
        "        return (self.emission_count.get(t_to_w, 0) + 1) / (self.tag_count.get(tag, 0) + len(self.vocab))\n",
        "\n",
        "    def viterbi_decode(self, sentence):\n",
        "        \"\"\" Viterbi decoding algorithm implementation\n",
        "\n",
        "        Arg:\n",
        "            sentence: a text sequence needed to be decoded\n",
        "        \"\"\"\n",
        "        viterbi = np.empty([len(self.tags), len(sentence)], dtype=float) # key = (tag, observation aka word in sentence)\n",
        "        backpointer = np.empty([len(self.tags), len(sentence)], dtype=int)\n",
        "        N = len(self.tags)\n",
        "        T = len(sentence)\n",
        "        for i in range(len(self.tags)):\n",
        "            tag = self.tags[i]\n",
        "            # print(tag)\n",
        "            # print(sentence[0])\n",
        "            # print(self.get_emission_prob(tag, sentence[0]))\n",
        "            viterbi[i, 0] = self.get_init_tag_prob(tag) * self.get_emission_prob(tag, sentence[0])\n",
        "            backpointer[i, 0] = -1 # Should this be a 0\n",
        "        for t in range(1, T):\n",
        "            word = sentence[t]\n",
        "            for s in range(N):\n",
        "                tag = self.tags[s]\n",
        "                max = float(\"-inf\")\n",
        "                max_idx = -999\n",
        "                for s_prime in range(N):\n",
        "                    tag_prime = self.tags[s_prime]\n",
        "                    curr = viterbi[s_prime, t-1] * self.get_trans_prob(tag_prime, tag) * self.get_emission_prob(tag, word)\n",
        "                    if curr > max:\n",
        "                        max = curr\n",
        "                        max_idx = s_prime\n",
        "                viterbi[s, t] = max\n",
        "                backpointer[s, t] = max_idx\n",
        "        max = float(\"-inf\")\n",
        "        max_idx = -999\n",
        "        for s in range(N):\n",
        "            tag = self.tags[s]\n",
        "            curr = viterbi[s, T-1] * self.get_trans_prob(tag, self.tags[N-1])\n",
        "            if curr > max:\n",
        "                max = curr\n",
        "                max_idx = s\n",
        "        bestpathprob = max\n",
        "        bestpathpointer = max_idx\n",
        "        bestpath = self.trace(bestpathpointer, backpointer)\n",
        "        return bestpath, bestpathprob\n",
        "    \n",
        "    def trace(self, bestpathpointer, backpointer):\n",
        "        states = []\n",
        "        s = bestpathpointer\n",
        "        # print('len(backpointer)', len(backpointer))\n",
        "        for t in range(len(backpointer[0]) - 1, -1, -1):\n",
        "            # print('s:', self.tags[s], 't:', t)\n",
        "            states.insert(0, self.tags[s])\n",
        "            s = backpointer[s, t]\n",
        "        return states\n",
        "\n",
        "\n",
        "sentences = load_corpus('./brown/')\n",
        "tagger = HMMTagger()\n",
        "tagger.initialize_probabilities(sentences)\n",
        "print(tagger.viterbi_decode(['the', 'planet', 'jupiter', 'and', 'its', 'moons', 'are', 'in', 'effect', 'a', 'mini', 'solar', 'system', '.']))\n",
        "print(tagger.viterbi_decode(['computers', 'process', 'programs', 'accurately', '.']))\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['DETERMINER', 'NOUN', 'NOUN', 'CONJUNCTION', 'PRONOUN', 'VERB', 'VERB', 'PREPOSITION', 'NOUN', 'DETERMINER', 'ADJECTIVE', 'ADJECTIVE', 'NOUN', 'PUNCT'], 2.8465742362225926e-49)\n",
            "(['DETERMINER', 'NOUN', 'NOUN', 'ADVERB', 'PUNCT'], 8.417180570032029e-23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFJvfGCPois_"
      },
      "source": [
        "import numpy as np # you may need this to convert lists to np arrays before returning them\n",
        "\n",
        "\n",
        "\n",
        "# Creates the dataset with train_X (words) and train_y (tag).\n",
        "def create_dataset(sentences):\n",
        "    # Defines the relevant lists.\n",
        "    train_words, train_tags = list(), list()\n",
        "    words = set([])\n",
        "    tags = set([])\n",
        "    for sentence in sentences:\n",
        "        sentence_X = []\n",
        "        sentence_y = []\n",
        "        for word_tag in sentence:\n",
        "            sentence_X.append(word_tag[0])\n",
        "            words.add(word_tag[0])\n",
        "            sentence_y.append(word_tag[1])\n",
        "            tags.add(word_tag[1])\n",
        "        train_words.append(np.array(sentence_X))\n",
        "        train_tags.append(np.array(sentence_y))\n",
        "        \n",
        " \n",
        "    # map each word to a unique integer\n",
        "    word_map = {word: idx + 2 for idx, word in enumerate(list(words))}    \n",
        "    word_map['-PAD-'] = 0\n",
        "    word_map['-OOV-'] = 1   \n",
        "\n",
        "    # Map each tag to a unique integer\n",
        "    tag_map = {tag: idx + 1 for idx, tag in enumerate(list(tags))}\n",
        "    tag_map['-PAD-'] = 0\n",
        "\n",
        "    train_X, train_y = list(), list()\n",
        "    # convert train_words to integers\n",
        "    for sentence in train_words:\n",
        "        sentence_ints = []\n",
        "        for word in sentence:\n",
        "            sentence_ints.append(word_map[word])\n",
        "        train_X.append(np.asarray(sentence_ints))\n",
        "    for sentence in train_tags:\n",
        "        sentence_ints = []\n",
        "        for word in sentence:\n",
        "            sentence_ints.append(tag_map[word])\n",
        "        train_y.append(np.asarray(sentence_ints))\n",
        "    \n",
        "    return word_map, tag_map, np.asarray(train_X), np.asarray(train_y)\n",
        "\n",
        "    # print(train_X)\n",
        "    # print(train_y)\n",
        "\n",
        "    # TODO: Your code goes here\n",
        "\n",
        "\n",
        "# Test the function here\n",
        "# Call create_dataset()\n",
        "# print(create_dataset([[('YOU', 'NOUN'), ('ARE','VERB'), ('.', 'PUNCT')], [('I', 'NOUN'), ('ARE','VERB'), ('!', 'PUNCT')], [('BYE', 'X'), ('.', 'PUNCT')]]))\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk0ZTHkvplxD"
      },
      "source": [
        "from tensorflow.python import train\n",
        "# from keras.preprocessing.sequence import pad_sequences as pad\n",
        "# from keras.utils import pad_sequences\n",
        "from keras.utils.data_utils import pad_sequences as pad\n",
        "\n",
        "# Pad the sequences with 0s to the max length.\n",
        "def pad_sequences(train_X, train_y):\n",
        "    # Use MAX_LENGTH to record length of longest sequence \n",
        "    # TODO: Your code goes here\n",
        "    MAX_LEN = len(max(train_X, key=len))\n",
        "    train_X = pad(train_X, maxlen=MAX_LEN, padding='post')\n",
        "    train_y = pad(train_y, maxlen=MAX_LEN, padding='post')\n",
        "    \n",
        "    return train_X, train_y, MAX_LEN\n",
        "\n",
        "# Test the function\n",
        "# train_X = np.asarray([np.asarray(['3', '4']), np.asarray(['5', '6'])])\n",
        "# train_y = np.asarray([np.asarray(['2', '3']), np.asarray(['4', '3'])])\n",
        "# train_X, train_y, MAX_LENGTH = pad_sequences(train_X, train_y)\n",
        "# print(train_X)\n",
        "# print(train_y)\n",
        "# print(MAX_LENGTH)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edy9gTV6qIhv"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import InputLayer, Activation\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Define the Keras model.\n",
        "def define_model(word_map, tag_map, MAX_LENGTH):  \n",
        "    model = Sequential()\n",
        "    # Add layer to receive input\n",
        "    model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "    # Compute word vector model for the words\n",
        "    model.add(Embedding(len(word_map), 128))\n",
        "    # Add LSTM layer. Bidirectional inputs the next values in the sequence\n",
        "    model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "    # Add a fully connected layer to pick the POS tag\n",
        "    model.add(TimeDistributed(Dense(len(tag_map))))\n",
        "    # Using softmax because this is a multi-class setting\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(0.001),\n",
        "                metrics=['accuracy'])\n",
        " \n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "# Call the function here\n",
        "# model = define_model(MAX_LENGTH)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0c2a7eUPQFi"
      },
      "source": [
        "# Returns the one-hot encoding of the sequence.\n",
        "def to_categorical(train_y, num_tags):\n",
        "    #TODO: Write code here\n",
        "    one_hot_y = []\n",
        "    for sent_idx in range(len(train_y)):\n",
        "        sentence = train_y[sent_idx]\n",
        "        one_hot_sentence = []\n",
        "        for tag_idx in range(len(sentence)):\n",
        "            tag = sentence[tag_idx]\n",
        "            # Get a list of zeros of len(num_tags)\n",
        "            zeros = np.zeros(num_tags)\n",
        "            one_hot_sentence.append(zeros)\n",
        "            # Change one of the zeros to a 1, so that it is a one-hot\n",
        "            one_hot_sentence[-1][tag] = 1.0\n",
        "        one_hot_y.append(one_hot_sentence)\n",
        "    return np.array(one_hot_y)\n",
        "\n",
        "# Call the function as to_categorical(train_y, categories = len(tag2idx))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN-Roc_AORIp"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Trains the model.\n",
        "def train(model, train_X, train_y):\n",
        "\n",
        "    # Fit the data into the Keras model, through 40 passes (epochs) using model.fit()\n",
        "    model.fit(train_X, to_categorical(train_y, len(tag_map)), batch_size=128, epochs=40, validation_split=.2)\n",
        "    # Return the model.\n",
        "    return model\n",
        "\n",
        "# call function here\n",
        "# model = train(model, train_X, train_y)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def from_categorical(seqs, idx):\n",
        "    tag_seqs = []\n",
        "    # Get each sequence of categorical tags\n",
        "    for cat_seq_idx in range(len(seqs)):\n",
        "        cat_seq = seqs[cat_seq_idx]\n",
        "        tag_seq = []\n",
        "        # Get each categorical tag\n",
        "        for cat_idx in range(len(cat_seq)):\n",
        "            cat = cat_seq[cat_idx]\n",
        "            # Convert the categorical tag back to its name\n",
        "            tag_seq.append(idx[np.argmax(cat)])\n",
        "        tag_seqs.append(tag_seq)\n",
        "    return tag_seqs"
      ],
      "metadata": {
        "id": "-77EprffD6AI"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANPb-K98i0w8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7098e97-c5e4-462c-dddf-c4906fd23ff3"
      },
      "source": [
        "from keras.utils.data_utils import pad_sequences as pad\n",
        "\n",
        "# Test a sentence using the given model.\n",
        "def test(model, word_map, tag_map, MAX_LEN, sentences):\n",
        "    # TODO: Write your code here\n",
        "    formatted_sents = []\n",
        "    for sentence in sentences:\n",
        "        formatted_sent = []\n",
        "        for word in sentence:\n",
        "            if word.lower() in word_map:\n",
        "                formatted_sent.append(word_map[word.lower()])\n",
        "            else:\n",
        "                formatted_sent.append(word_map['-OOV-'])\n",
        "        formatted_sents.append(formatted_sent)\n",
        "    formatted_sents = pad(formatted_sents, maxlen=MAX_LEN, padding='post')\n",
        "\n",
        "    preds = model.predict(formatted_sents)\n",
        "    pred_tags = from_categorical(preds, {i: t for t, i in tag_map.items()})\n",
        "    no_pads = []\n",
        "    for sentence in pred_tags:\n",
        "        sent_no_pads = []\n",
        "        for tag in sentence:\n",
        "            if not tag == '-PAD-':\n",
        "                sent_no_pads.append(tag)\n",
        "        no_pads.append(sent_no_pads)\n",
        "    for sent_idx in range(len(no_pads)):\n",
        "        print()\n",
        "        print(sentences[sent_idx])\n",
        "        print(no_pads[sent_idx])\n",
        "    \n",
        "\n",
        "sentences = load_corpus('./brown/')\n",
        "word_map, tag_map, train_X, train_y = create_dataset(sentences)\n",
        "train_X, train_y, MAX_LEN = pad_sequences(train_X, train_y)\n",
        "model = define_model(word_map, tag_map, MAX_LEN)\n",
        "model = train(model, train_X, train_y)\n",
        "\n",
        "# For the first evaluation sentence.\n",
        "# testString1 = [\"the\", \"secretariat\", \"is\", \"expected\", \"to\", \"race\", \"tomorrow\", \".\"]\n",
        "# call test() to print tags\n",
        "\n",
        "# For the second evaluation sentence.\n",
        "# testString2 = \"people continue to enquire the reason for the race for outer space .\"\n",
        "# call test() to print tags\n",
        "test_sents = [['the', 'planet', 'jupiter', 'and', 'its', 'moons', 'are', 'in', 'effect', 'a', 'mini', 'solar', 'system', '.'], ['computers', 'process', 'programs', 'accurately', '.']]\n",
        "test(model, word_map, tag_map, MAX_LEN, test_sents)\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-21f4ae5f5a15>:45: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return word_map, tag_map, np.asarray(train_X), np.asarray(train_y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 180, 128)          6367104   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 180, 512)         788480    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, 180, 12)          6156      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 180, 12)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,161,740\n",
            "Trainable params: 7,161,740\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/40\n",
            "359/359 [==============================] - 26s 64ms/step - loss: 0.1933 - accuracy: 0.9445 - val_loss: 0.0357 - val_accuracy: 0.9902\n",
            "Epoch 2/40\n",
            "359/359 [==============================] - 23s 64ms/step - loss: 0.0162 - accuracy: 0.9953 - val_loss: 0.0140 - val_accuracy: 0.9955\n",
            "Epoch 3/40\n",
            "359/359 [==============================] - 22s 60ms/step - loss: 0.0080 - accuracy: 0.9974 - val_loss: 0.0121 - val_accuracy: 0.9960\n",
            "Epoch 4/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 0.0060 - accuracy: 0.9980 - val_loss: 0.0117 - val_accuracy: 0.9962\n",
            "Epoch 5/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 0.0120 - val_accuracy: 0.9961\n",
            "Epoch 6/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.0131 - val_accuracy: 0.9961\n",
            "Epoch 7/40\n",
            "359/359 [==============================] - 22s 63ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.0130 - val_accuracy: 0.9961\n",
            "Epoch 8/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0134 - val_accuracy: 0.9962\n",
            "Epoch 9/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0142 - val_accuracy: 0.9961\n",
            "Epoch 10/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0155 - val_accuracy: 0.9960\n",
            "Epoch 11/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.0176 - val_accuracy: 0.9956\n",
            "Epoch 12/40\n",
            "359/359 [==============================] - 23s 63ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0177 - val_accuracy: 0.9958\n",
            "Epoch 13/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 8.2496e-04 - accuracy: 0.9998 - val_loss: 0.0191 - val_accuracy: 0.9957\n",
            "Epoch 14/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 6.5604e-04 - accuracy: 0.9998 - val_loss: 0.0199 - val_accuracy: 0.9956\n",
            "Epoch 15/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 5.4307e-04 - accuracy: 0.9999 - val_loss: 0.0212 - val_accuracy: 0.9957\n",
            "Epoch 16/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 4.2676e-04 - accuracy: 0.9999 - val_loss: 0.0218 - val_accuracy: 0.9956\n",
            "Epoch 17/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 3.3314e-04 - accuracy: 0.9999 - val_loss: 0.0232 - val_accuracy: 0.9956\n",
            "Epoch 18/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 2.9388e-04 - accuracy: 0.9999 - val_loss: 0.0248 - val_accuracy: 0.9954\n",
            "Epoch 19/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 2.7117e-04 - accuracy: 0.9999 - val_loss: 0.0250 - val_accuracy: 0.9954\n",
            "Epoch 20/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 2.4688e-04 - accuracy: 0.9999 - val_loss: 0.0259 - val_accuracy: 0.9954\n",
            "Epoch 21/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 2.2955e-04 - accuracy: 0.9999 - val_loss: 0.0272 - val_accuracy: 0.9953\n",
            "Epoch 22/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 2.1676e-04 - accuracy: 0.9999 - val_loss: 0.0270 - val_accuracy: 0.9955\n",
            "Epoch 23/40\n",
            "359/359 [==============================] - 22s 63ms/step - loss: 2.5916e-04 - accuracy: 0.9999 - val_loss: 0.0272 - val_accuracy: 0.9955\n",
            "Epoch 24/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 1.5397e-04 - accuracy: 1.0000 - val_loss: 0.0279 - val_accuracy: 0.9955\n",
            "Epoch 25/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 9.7040e-05 - accuracy: 1.0000 - val_loss: 0.0282 - val_accuracy: 0.9955\n",
            "Epoch 26/40\n",
            "359/359 [==============================] - 23s 63ms/step - loss: 7.3852e-05 - accuracy: 1.0000 - val_loss: 0.0297 - val_accuracy: 0.9954\n",
            "Epoch 27/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 8.0196e-05 - accuracy: 1.0000 - val_loss: 0.0297 - val_accuracy: 0.9954\n",
            "Epoch 28/40\n",
            "359/359 [==============================] - 22s 63ms/step - loss: 1.0815e-04 - accuracy: 1.0000 - val_loss: 0.0309 - val_accuracy: 0.9953\n",
            "Epoch 29/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 1.9774e-04 - accuracy: 0.9999 - val_loss: 0.0306 - val_accuracy: 0.9954\n",
            "Epoch 30/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 1.7828e-04 - accuracy: 0.9999 - val_loss: 0.0310 - val_accuracy: 0.9954\n",
            "Epoch 31/40\n",
            "359/359 [==============================] - 23s 63ms/step - loss: 1.0636e-04 - accuracy: 1.0000 - val_loss: 0.0309 - val_accuracy: 0.9955\n",
            "Epoch 32/40\n",
            "359/359 [==============================] - 22s 62ms/step - loss: 6.6572e-05 - accuracy: 1.0000 - val_loss: 0.0324 - val_accuracy: 0.9955\n",
            "Epoch 33/40\n",
            "359/359 [==============================] - 22s 62ms/step - loss: 3.7969e-05 - accuracy: 1.0000 - val_loss: 0.0317 - val_accuracy: 0.9955\n",
            "Epoch 34/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 2.3899e-05 - accuracy: 1.0000 - val_loss: 0.0332 - val_accuracy: 0.9955\n",
            "Epoch 35/40\n",
            "359/359 [==============================] - 23s 63ms/step - loss: 1.8504e-05 - accuracy: 1.0000 - val_loss: 0.0332 - val_accuracy: 0.9955\n",
            "Epoch 36/40\n",
            "359/359 [==============================] - 22s 62ms/step - loss: 1.2339e-05 - accuracy: 1.0000 - val_loss: 0.0336 - val_accuracy: 0.9956\n",
            "Epoch 37/40\n",
            "359/359 [==============================] - 22s 62ms/step - loss: 3.7297e-05 - accuracy: 1.0000 - val_loss: 0.0347 - val_accuracy: 0.9955\n",
            "Epoch 38/40\n",
            "359/359 [==============================] - 22s 62ms/step - loss: 3.0402e-04 - accuracy: 0.9999 - val_loss: 0.0333 - val_accuracy: 0.9954\n",
            "Epoch 39/40\n",
            "359/359 [==============================] - 23s 64ms/step - loss: 1.8677e-04 - accuracy: 0.9999 - val_loss: 0.0320 - val_accuracy: 0.9955\n",
            "Epoch 40/40\n",
            "359/359 [==============================] - 22s 61ms/step - loss: 7.0404e-05 - accuracy: 1.0000 - val_loss: 0.0335 - val_accuracy: 0.9954\n",
            "1/1 [==============================] - 1s 835ms/step\n",
            "\n",
            "['the', 'planet', 'jupiter', 'and', 'its', 'moons', 'are', 'in', 'effect', 'a', 'mini', 'solar', 'system', '.']\n",
            "['DETERMINER', 'NOUN', 'NOUN', 'CONJUNCTION', 'PRONOUN', 'NOUN', 'VERB', 'PREPOSITION', 'VERB', 'DETERMINER', 'ADJECTIVE', 'ADJECTIVE', 'NOUN', 'PUNCT']\n",
            "\n",
            "['computers', 'process', 'programs', 'accurately', '.']\n",
            "['VERB', 'NOUN', 'NOUN', 'ADVERB', 'PUNCT']\n"
          ]
        }
      ]
    }
  ]
}